---
layout: post
title: How to Build a Neural Network from a Bucket of Water
description: "An approach to neural network training from the beforetimes"
cover-img: "/assets/img/far_philly.jpg"
---
# Intro
Many characteristics of deep learning are assumed to be requirements carved in stone because they are ubiquitous.
It seems intuitively obvious that one should train their whole neural network, so people do it without question.
However, neural networks work surprisingly well even after major changes.
In this blog post, I'm going to talk about the paper [Pattern Recognition in a Bucket](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.97.3902), 
resevoir computing, and why any of it matters since we have GPUs.


## Pattern Recognition in a Bucket
### Motivation 
Neural networks are hard to train, especially if you don't have GPUs and autograd software.

- What did they do

- Why did they do that?
There is an entire field called resevoir computing that is characterized by training only a linear output layer of a large high-dimensional system.
One such method is called a liquid state machine, and the authors of this paper set out to make a liquid state machine out of liquid.
- Why resevoir computing mattered then and why people still use it
- Liquid state machines and their relationship to 

## So What?
- Several recent papers have been looking into this idea from a modern DL perspective. (Lottery ticket, 
- Offloading computation to nature (tie-in encryption lava lamps? https://www.cloudflare.com/learning/ssl/lava-lamp-encryption/)
- Debugging neural nets is hard; they'll do well even if you're only training one layer
- Difference in ML funding between 2003 and now is striking

## Takeaways:
- Old ideas in DL often become useful again
- Don't take anything in DL for granted
