---
layout: post
title: How to Build a Neural Network from a Bucket of Water
description: "An approach to neural network training from the beforetimes"
cover-img: "/assets/img/traintracks.jpg"
---
# Intro
Many characteristics of deep learning are assumed to be requirements carved in stone because they are ubiquitous.
It seems intuitively obvious that one should train their whole neural network, so people do it without question.
However, neural networks work surprisingly well even after major changes.
In this blog post, I'm going to talk about the paper [Pattern Recognition in a Bucket](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.97.3902), 
reservoir computing, and why any of it matters since we have GPUs.


## Pattern Recognition in a Bucket
### Motivation 
Neural networks are hard to train, especially if you don't have GPUs and autograd software.
In the early 2000s [Torch had just been released](https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.8.9850),
GPUs were orders of magnitude weaker than they are today, and the best way to train recurrent neural networks [was still up for debate](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.21.3096).
A couple groups found some success by cutting this Gordian Knot: they said "if training RNNs is hard, then just don't do it!"
More specifically, they came up with the idea of [liquid state machines](https://pubmed.ncbi.nlm.nih.gov/12433288/) and [echo state networks](https://papers.nips.cc/paper/2002/file/426f990b332ef8193a61cc90516c1245-Paper.pdf).
These papers point out that if you have enough untrained recurrent units acting as a reservoir of potential functions, then you can simply train a linear function of their outputs to solve your problem of choice.
Collectively, this framework is now referred to as [reservoir computing](https://en.wikipedia.org/wiki/Reservoir_computing).

Interestingly, the Liquid State Machines paper doesn't limit itself to only using recurrent neural networks.
The authors show that any dynamic system that meets certain requirements (glossed over here) can be fed into a linear layer and used to learn things.
Fernando and Sojakka, the authors of Pattern Recognition in a Bucket, read the Liquid State Machines paper and said "You know what's a liquid? Water!"
They then set out on the quest to to train a neural network[^network] with a bucket of water.

### What did they do
#### Setup
To feed inputs into a reservoir, the authors used a set of motors to create waves in a tank of water (shown below). 
The output was created by shining a light through the water onto cardboard, taking a video of the cardboard, running a filter over the output, then feeding the resulting pixels into a perceptron.

{% include responsive-image.html image="assets/img/liquid_brain.png" %}

This figure is one of my favorites in any paper I've read.
It's hard to beat a plastic box covered in LEGO motors with arrows drawn on in MSPaint.
The caption "The Liquid Brain" is just the cherry on top[^liquid].

#### Experiments
As a proof-of-concept, the authors first used their Liquid Brain to solve the XOR problem.
For context, XOR is a simple binary classification problem where you return one if both inputs are 0 or 1, and 0 otherwise.
XOR is a good starting place, because it's easy to solve with a nonlinear system ([or just an extra input dimension](http://www.cs.cmu.edu/~tom/10601_fall2012/recitations/Recitation_2Oct2012_PG.pdf)), but impossible to solve with a linear model.
The complexity of the liquid state machine is overkill for solving XOR, which the authors acknowledge, saying "Our approach of adding an extra 700 dimensions is to use a machete to garnish canapes."
Mostly the XOR experiment is just there to show that the model actually works.

To show that their system would work on a more challenging problem, Fernando and Sojakka then went on to tackle speech recognition.
More specifically, they convert the waveforms of an individual saying "zero" and "one" were converted to motor frequencies via a flavor of Fourier transform.
This process was repeated for 20 iterations of each word, and the results were fed into a perceptron.
Surprisingly (to me at least), they managed to get ~99 percent training accuracy as opposed to 75 percent accuracy when training the perceptron on the raw input.

## So What?
Like most papers, this paper did not spark an "[ImageNet Moment](https://ruder.io/nlp-imagenet/)".
[People](https://www.degruyter.com/document/doi/10.1515/nanoph-2016-0132/html) [still](http://materias.df.uba.ar/dnla2019c1/files/2019/03/reservoir_computing_20191.pdf) [use](https://arxiv.org/abs/1808.04962)
reservoir computing, but the field lies far off the beaten path of machine learning research.

Despite it being less well known than more mainstream approaches, ideas from reservoir computing show up frequently in deep learning research. 
For example, the idea that a neural network has useful functions baked in at initialization time is at the core of the [Lottery Ticket Hypothesis](https://arxiv.org/abs/1803.03635).
Similarly, [Rosenfeld and Tsotsos](https://arxiv.org/abs/1802.00844) found that the final layer isn't special: you can train a random subset of weights in a neural network and still get good performance.

I believe all these papers (along with [papers about pruning](https://arxiv.org/abs/2003.03033)) are approaching the same phenomenon from different directions.
Something to do with the large number of functions that are represented at initialization, 
or the dynamics of neural network training, 
or some aspect of random matrix theory, 
causes neural networks to be robust to lots of modifications that intuition says should break them.

## Takeaways:
- Old ideas in deep learning often become useful again.
- Aspects of deep learning that seem critical (like training all the weights) often aren't.

---

#### Interesting Historical Notes
- Back in 2003 when Pattern Recognition in a Bucket was first published, a [high end graphics card](https://www.anandtech.com/show/1183) would set you back 250 dollars and have ~ 10 shaders.
Today's [top of the line graphics card](https://www.nvidia.com/en-us/geforce/graphics-cards/30-series/rtx-3090/) has ~10000 shaders and has a price of around $2000, though it fluctuates based on semiconductor tariffs, cryptocurrency demand, and profiteering due to the small supply.

- [This review paper](https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2007-8.pdf) is fascinating in how it shows that there are many potential 
ways to make neural networks work, and that it was not at all clear in 2007 that autodiff + backpropogation + GPUs + large datasets would be the dominant paradigm.

#### Footnotes

[^liquid]: I miss showy method names like "The Liquid Brain", "Neocognitron", or "Perceptron", though modern names are also pretty good.
[^network]: It's technically a liquid state machine, but I'll count it because LSMs are typically done with neural network layers.
