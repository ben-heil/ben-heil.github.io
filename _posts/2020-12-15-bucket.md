---
layout: post
title: How to Build a Neural Network from a Bucket of Water
description: "An approach to neural network training from the beforetimes"
cover-img: "/assets/img/far_philly.jpg"
---
# Intro
Many characteristics of deep learning are assumed to be requirements carved in stone because they are ubiquitous.
It seems intuitively obvious that one should train their whole neural network, so people do it without question.
However, neural networks work surprisingly well even after major changes.
In this blog post, I'm going to talk about the paper [Pattern Recognition in a Bucket](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.97.3902), 
reservoir computing, and why any of it matters since we have GPUs.


## Pattern Recognition in a Bucket
### Motivation 
Neural networks are hard to train, especially if you don't have GPUs and autograd software.
In the early 2000s [Torch had just been released](https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.8.9850),
GPUs were orders of magnitude weaker than they are today, and the best way to train recurrent neural networks [was still up for debate](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.21.3096).
A couple groups found some success by cutting this Gordian Knot: they said "if training RNNs is hard, then just don't do it!"
More specifically, they came up with the idea of [liquid state machines](https://pubmed.ncbi.nlm.nih.gov/12433288/) and [echo state networks](https://papers.nips.cc/paper/2002/file/426f990b332ef8193a61cc90516c1245-Paper.pdf).
These papers point out that if you have enough untrained recurrent units acting as a reservoir of potential functions, then you can simply train a linear function of their outputs to solve your problem of choice.

Interestingly, the Liquid State Machines paper doesn't limit itself to only using recurrent neural networks.
The authors show that any dynamic system that meets certain requirements (glossed over here) can be fed into a linear layer and used to learn things.
The authors of Pattern Recognition in a Bucket read the Liquid State Machines paper and said "You know what's a liquid? Water!"
They then set out on their quest to to train a neural network TODO footnote(technically a liquid state machine, but I'll count it because LSMs are typically done with neural network layers) with a bucket of water.

### What did they do
#### Setup
To feed inputs into a reservoir, the authors used a set of motors to create waves in a tank of water (shown below). 
The output was created by shining a light through the water onto cardboard, taking a video of the cardboard, running a filter over the output, then feeding the resulting pixels into a perceptron.
TODO image

This figure one of my favorite figures in any paper I've ever read.
It's hard to beat a plastic box covered in LEGO motors propped up by an optics textbook.
#### The XOR Problem
The 


## Speech Recognition

- Why did they do that?
There is an entire field called reservoir computing that is characterized by training only a linear output layer of a large high-dimensional system.
One such method is called a liquid state machine, and the authors of this paper set out to make a liquid state machine out of liquid.
- Why reservoir computing mattered then and why people still use it
- Liquid state machines and their relationship to 

## So What?
- Several recent papers have been looking into this idea from a modern DL perspective. (Lottery ticket, 
- Offloading computation to nature (tie-in encryption lava lamps? https://www.cloudflare.com/learning/ssl/lava-lamp-encryption/)
- Debugging neural nets is hard; they'll do well even if you're only training one layer
- Difference in ML funding between 2003 and now is striking

## Takeaways:
- Old ideas in DL often become useful again
- Don't take anything in DL for granted

## Interesting Historical Notes
- Back in 2003 when Pattern Recognition in a Bucket was first published, a [high end graphics card](https://www.anandtech.com/show/1183) would set you back 250 dollars and have ~ 10 shaders.
Today's [top of the line graphics card](https://www.nvidia.com/en-us/geforce/graphics-cards/30-series/rtx-3090/) has ~10000 shaders and has a price of around $2000, though it fluctuates based on semiconductor tariffs, cryptocurrency demand, and profiteering due to the small supply.
What a world we live in
- [This review paper](https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2007-8.pdf) is fascinating in how it shows that there are many potential 
ways to make neural networks work, and that it was not at all clear in 2007 that autodiff + backpropogation + GPUs + large datasets would be the dominant paradigm.

