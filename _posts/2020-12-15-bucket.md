---
layout: post
title: How to Build a Neural Network from a Bucket of Water
description: "A brief journal club on Pattern Recognition in a Bucket"
cover-img: "/assets/img/far_philly.jpg"
---
# Intro
As you stray from the beaten path of popular publications, you begin to find fascinating papers that challenge your intuition about how deep learning works.
My favorite ideas tend to come from papers that move in different directions from the field as a whole.
To share those ideas with others, I've decided to do occasional journal club posts where I talk about fun papers.
This post is the first in that series, so we'll se how it goes.

## Motivation
Neural networks are hard to train, especially if you don't have GPUs and autograd software.
One way around that issue is to avoid training the whole network.
In fact there is an entire family of methods called [resevoir computing](#resevoir) that is characterized by training only a linear output layer of a large high-dimensional system.
One such method is called a liquid state machine, and the authors of this paper set out to make a liquid state machine out of liquid.

## Background

<a id='resevoir'></a>
### Resevoir computing
- Why resevoir computing mattered then and why people still use it
- Liquid state machines and their relationship to 

## Why this is interesting
- Offloading computation to nature (tie-in encryption lava lamps? https://www.cloudflare.com/learning/ssl/lava-lamp-encryption/)
- Debugging neural nets is hard; they'll do well even if you're only training one layer
- Difference in ML funding between 2003 and now is striking
- Neural nets start training as random matrices, and random matrix theory is used a lot in moder theoretical DL work

## Practical applications:
Probably none
