---
layout: post
title: The Past, Present, and Future of Academic Publishing
description: ""
cover-img: "/assets/img/traintracks.jpg"
tags: ['grad school', 'academia', 'publication']
---

## The Past

The scientific publication system is simple.
Once scientists have written a paper about their science, they send their work to a company called a publisher.
The publisher then gives other scientists (called reviewers) [internet points](https://publons.com/about/home/) to determine whether it is worth their time to publish.
If the reviewers decide the paper is good enough, the publisher will generously allow the authors to pay the journal to put it on their website.
There is, of course, another fee if the authors want people to be able to read their paper without paying.

It wasn't always this way though.

Historically, scientists communicated [to the public through books and to each other with letters](http://garfield.library.upenn.edu/essays/v4p394y1979-80.pdf).
Then, around the 17th century, scientific societies were formed to "[share their results in person, and presumably consume alcohol](https://blogs.scientificamerican.com/information-culture/the-mostly-true-origins-of-the-scientific-journal/)".
Over time, these societies began to publish periodicals, which were the precursors to modern academic journals.
However, the publication system we have today is largely a result of the expansion and codification of academia in the decades after WWII[^fyfe].

The present-day academic publication system is inefficient, and the [costs](https://twitter.com/DGlaucomflecken/status/1484679759829209090) of this inefficiency are largely borne by scientists.
Fortunately, with the advent of the internet several alternatives have arisen.
This post discusses [issues](https://twitter.com/behrenstimb/status/1492269544513150977) in the current system of publication and what the future of academic publishing might look like.

## The Present

**Paper Ownership**  
When a paper is published in closed-access journals, the publisher owns[^owns] the paper.
In the past, this meant that only people who paid the journal's access fees were able to read papers.
After pushback (and an [NIH mandate](https://en.wikipedia.org/wiki/NIH_Public_Access_Policy)) it is now possible to read some publicly funded research for free (after an embargo) through PubMed Central.
The publisher still keeps the copyright, of course, so [only a subset](https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/) of published articles are available for future research.

Even paying an institutional licensing fee doesn't give you free reign.
[Chris Albon once tried](http://partiallyderivative.com/podcast/2017/02/09/mini-episode-data-scraping) to scrape JSTOR articles to a tablet so he could spend a summer backpacking and preparing for his comprehensive exams.
As those familiar with the publishing system would predict, JSTOR threatened sanctions against his university and made him delete all the articles.
Posession of large numbers of academic articles will cause people to show up at your office. 
Possession with the intent to distribute invariably incurs [legal](https://en.wikipedia.org/wiki/Aaron_Swartz) [action](https://www.theverge.com/2018/2/8/16985666/alexandra-elbakyan-sci-hub-open-access-science-papers-lawsuit).

**Publication Delays**  
If all goes well scientists write a paper, submit it, wait, get reviews, address them, wait, then publish.
In biology, the median time between having a paper complete enough to preprint and having the paper published is about [150 days](https://www.biorxiv.org/content/10.1101/2021.03.04.433874v2.full.pdf).
During this time the research is done and the paper is written, but no one other than the authors, reviewers, and the publisher can read it[^preprints].
As a result, the rate of "official" scientific progress is slowed down by months.

**Peer Review**  
Modern peer review is a little strange, in that it involves for-profit companies getting consulting hours from scientists with advanced degrees for free.
I'm of the opinion that this is bad, and not just because I'm a grad student who likes having money.
When making a resource free (or very cheap) any system attempting to maximize profits will use it for [anything they can draw value from](https://twitter.com/SwiftOnSecurity/status/1074856487514726400?s=20&t=BV4kfDhsriljMwkR-F9T9w).
While peer review itself isn't a resource that broadly useful, academic labor is.
If we're not careful about giving away scientist-hours for free, we might end up with universities posting unpaid professor positions or something.
[Oh wait](https://twitter.com/kchoudhu/status/1505040603851239424).

The current framing of peer review is also flawed.
Publishers aren't trying to determine how good a paper is, they're trying to determine [whether they should publish it](https://authorservices.wiley.com/Reviewers/journal-reviewers/what-is-peer-review/index.html).
In an ideal world, these would be the same thing, but in practice they diverge due to perverse incentives.

Authors want their paper to be published after one round of review so that they can spend their time doing science instead of corresponding with peer reviewers.
Accordingly, the acceptable response to reviews takes the form "[We thank the reviewer](https://openreview.net/search?term=%22we+thank+the+reviewer%22&group=all&content=all&source=all) for their suggestion that we [cite their marginally related articles](https://academic.oup.com/bioinformatics/article/35/18/3217/5304360), and have done so in the revised manuscript."
Disagreements are heavily penalized as they increase the probability of a negative assessment from the reviewer and a second round of suggestions.

Reviewers, knowing this, can get away with recommending that the authors cite their paper.
Alternatively, they can emphasize all the flaws in their competitors' paper in an attempt to keep it from getting published.

Because reviewers have no skin in the game, it's challenging to put checks on this kind of malfeasance.
The worst case scenario is that they get reported and removed from their unpaid job as a reviewer for that journal.

These perverse incentives lead to a system where peer review [infrequently](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3001285) changes the results of a paper, but frequently [causes delays](https://www.nature.com/articles/s41599-021-00920-9/figures/1).

**Externalized Costs**  
One reason that the inefficiencies in the modern publishing system aren't more visible is that many of the costs are [externalized](https://en.wikipedia.org/wiki/Cost_externalizing).
For example the cost of paper access is paid for by taxpayers/nonprofits by way of universities' [indirect costs](https://oamp.od.nih.gov/division-of-financial-advisory-services/indirect-cost-branch/indirect-cost-submission/indirect-cost-definition-and-example) on grants.

Academics generally aren't presented with the cost to access a journal, unless their [university system](https://www.insidehighered.com/news/2019/03/01/university-california-cancels-deal-elsevier-after-months-negotiations) or [country](https://www.editage.com/insights/norway-joins-the-ranks-of-germany-and-sweden-cancels-subscription-with-elsevier) decides to fight a publisher.
They simply log in from a university IP address or use a VPN and get automatic access.
Meanwhile, taxpayers funding the research have to hope the article they want is open access, wait for the article to show up on PubMed Central, pay hundreds of dollars for a subscription to the journal, or pirate the paper from site like SciHub.

Likewise, the costs of peer review are borne by academics.
As things stand now, peer review is a system where world-class experts provide [billions of dollars](https://researchintegrityjournal.biomedcentral.com/articles/10.1186/s41073-021-00118-2) of consulting to for-profit publishing companies for free.

A less-discussed externalized cost of the modern publication system is the increased risk to the scientific record.
Academic publishers own the copyright for the articles they publish, so it is illegal to create an archive of scientific literature.
As a result, there is a single point of failure for the preservation of scientific knowledge.
Publishers surely have backup systems, but how robust are they?
If a smaller journal got [ransomwared](https://www.pbs.org/newshour/nation/why-ransomware-attacks-are-on-the-rise-and-what-can-be-done-to-stop-them), would they have the funds to pay the ransom?
Or would that science be lost forever?

## The Future
Flaws aside, modern academic journals play several important roles.

For authors journals act as platforms, allowing scientists a place to disseminate their ideas.
Even if the authors of a paper aren't widely known, if they can write a paper good[^good] enough to get into e.g. Nature or Science, it will get readership.
While increasing readership of certain papers isn't the goal of science per se, citations are a unit of academic currency and journals help mitigate the extent to which academic success is driven solely by networking ability.

For readers journals act as filters, separating "can't miss" papers from the pack.
[Thousands of biological papers](https://www.biorxiv.org/search/jcode%3Abiorxiv%20limit_from%3A2020-01-01%20limit_to%3A2020-12-31%20numresults%3A10%20sort%3Arelevance-rank%20format_result%3Astandard) are posted each month, making it challenging for scientists to keep track of which papers are relevant, much find the best ones.
Journals coordinate peer review as an additional filtering mechanism to augment the editor's discernment with that of experts in the field.

I don't believe in providing criticism without alternatives unless you genuinely believe "nothing" is a better option than the thing you're arguing against.
So how do we achieve the roles played by academic journals while reducing the costs?

### Platforms  
While in the past the only ways for scientists to disseminate work was through journals, letters, or conferences, the internet has made distribution much easier.
The number of papers being submitted to preprint servers is [growing rapidly](https://asapbio.org/preprint-info/biology-preprints-over-time), allowing scientists to share their work with the world for free (!).
While preprint servers don't necessarily have the same cultural prestige as major journals, removing the barrier to actually reading the work in question greatly expands what is possible.

For example, deep learining research uses a hybrid model of preprints and conferences[^disclaimer].
Deep learning papers tend to be published as preprints when they are completed,[^embargo] then submitted to conferences instead of journals ([if they are submitted at all](https://twitter.com/fchollet/status/1467023443010928640)).
Conferences like NeurIPS and ICML are the equivalents of Cell or Nature in biology, and are paid close attention to by the field as a whole.
While the conference system [has its](https://www.reddit.com/r/compsci/comments/by4w3/the_cs_conference_reviewing_process_is_broken/) [own problems](https://www.researchgate.net/post/What_is_your_opinion_on_the_current_conference-driven_publication_model_in_CS), it's at least an order of magnitude cheaper for scientists.

Another platform of note is science Twitter.
Though I don't think the format is actually healthy for communicating science (or anything for that matter), it is effective at transmitting ideas to a large number of people.
Some day I want to run an experiment by purchasing a twitter ad for the same price as an article publication fee and see whether the journal or the ad gets more people to read the paper.
I suspect it would be the latter.

Twitter isn't designed to be a platform for discussing and circulating scientific research, but some websites tackle at least parts of that goal.
[Publons](https://publons.com/about/home/), for example, aims to give more credit to peer reviewers.
Due to anonymity, peer reviews historically haven't been rewarded.
Publons gives scientists the ability to point to a trusted source and say "I reviewed X articles in Y journals over the past five years."

### Filters
Every scientific paper is different, and each paper is of more or less interest depending on the person.
Given infinite time, scientists would try to read all papers in existence, but we don't live in that world.
Instead, we try to read as many papers directly pertaining to our research topic as we can while reading only the more important papers for less related topics[^reading].
As things stand, journals are organized to fit this system well.
There are journals dedicated solely to (in their opinion) papers of high general importance, and other journals that focus on the minutiae of a specific topic.

Requiring scientists to skim all the preprinted papers to stay up to date in their field is likely an impractical amount of work.
To avoid this fate, we need to replicate two key functions: aggregation ("what is relevant?"), and peer review ("what is good?").

**Alternate aggregation**  
Speaking of impractical amounts of work, let's talk about deep learning research as a field.
Because of its conference-based system and the fast pace of progress, scientists in deep learning actually are reliant on preprints in a field that posts hundreds per day.
To survive such a dynamic environment, deep learning communities have responded by building tools to make the firehose of information more manageable.
As a result, several of the best methods for aggregation I've seen so far come from the deep learning comunity.

Ironically, given the field's drive to automate tasks done by humans, one of the current solutions is to throw scientist-hours at the problem.
Freelance paper aggregators such as [AK](https://twitter.com/ak92501) and [Aran Komatsuzaki](https://twitter.com/arankomatsuzaki)[^AK] select the preprints that they think are the most important each day and post about them on Twitter.

A more formal implementation of this same idea is something called an '[overlay journal](https://peeriodicals.com/)'.
Overlay journals are called such because they build on top of the existing publication infrastructure.
Instead of handling peer review, hosting, and so on, overlay journals simply aggregate papers on a certain topic as they are preprinted or published elsewhere.
In the overlay paradigm, individuals are able to create a collection of papers relevant to their field without the costs of starting their own publication system.

Off the beaten path, you can see more creative views of science publication.
Creators like [3Blue1Brown](https://www.youtube.com/watch?v=3s7h2MHQtxc) show that you can learn science by seeing it instead of by reading it.
Embracing that thesis, channels like [OMGenomics](https://www.youtube.com/watch?v=-PCKK_nwFdA) and [Yannic Kilcher](https://www.youtube.com/watch?v=iDulhoQ2pro) have sprung up to do walkthroughs of scientific projects and papers.
Instead of just selecting papers that are interesting, such channels add their own interpretation and understanding to the science.

While the methods I've presented so far have all involved manual curation, there are certainly other ways of approaching the aggregation problem.
[Several](https://arxiv-sanity-lite.com/about) [natural language processing](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0158423) [methods](https://elicit.org) exist for finding and aggregating papers.
Such methods are able to take in a larger quantity of papers than a human could, and may be less swayed by a publication's venue or the status the authors writing the paper.
The hard part, as always, lies in getting the algorithm's results to align with intent of the human driving it.

Since paper aggregation is largely a matter of taste, I don't think there is a single answer for what the best alternative aggregation system is.
My guess is that the future will be made up of several different manual and automatic systems, driven by popularity and sutability for particular uses.

**Peer review**  
Although peer review is a useful signal[^yelp], in its [current form](https://www.youtube.com/watch?v=lt_b4VKBDhI) is inefficient and misaligned with the goals of science.

One potential method for aligning the incentives of peer reviewers, authors, and publishers would be to [pay peer reviewers](https://jamesheathers.medium.com/the-450-movement-1f86132a29bd).
Paying reviewers both encourages publishers to value reviewers' time by avoiding sending papers for peer review that are unlikely to be accepted, and encourages reviewers to behave well to avoid risking their payment.
You could even imagine a system where grad students looking to augment their stipends and gain domain knowledge could increase their peer-reviewing rather than teaching classes.

Another promising way to improve the review process would be to separate it from publishers' decisions to accept or reject the paper.
The peer review process [takes months](https://link.springer.com/article/10.1007/s11192-017-2310-5#Sec7), and having to rereview a paper due to journal fit or expected impact exacerbates the issue.
[Review Commons](https://www.reviewcommons.org/) is developing a way to preventing rereviews (at least for the life sciences).
They coordinate peer review, then send the resulting reviews to their affiliate journals.
While they still reject papers based on [topic scope and perceived importance](https://www.reviewcommons.org/faq/), Review Commons is still moving the field towards journal-agnostic peer review, which is a win in my book.

While Review Commons makes the peer review process more efficient by coordinating journals, [OpenReview](https://openreview.net/) has gone a step further by building an entire platform.
At the moment, this platform is still very tied to individual conferences; works are submitted to a conference and reviewed/accepted/rejected by people affiliated with the conference.
The idea of a peer review platform promises more, though.

With very similar technology, it would be possible to coordinate freelance paid peer review (thankfully, OpenReview is open source).
The majority of peer review today is anonymous and fragmented.
Each journal knows the tone and quality of individuals' reviews, but there are so many journals it is difficult to get an idea of reviewers' calibration[^calibration].
Meanwhile, [Publons](https://publons.com/about/home/) knows who has reviewed for which papers, but not the scores they gave or the content of the reviews.
By centralizing reviews and reviewers a platform could simultaneously shine light on bad behavior, disentangle the peer review and journal acceptance process, and coordinate reviewer payment.
As an added bonus, by making the system pseudonymous such a platform could achieve all these goals while maintaining single- or double-blind peer review.


### The article of the future
As the world continues to grow and change the systems around academic publication will change with it.
But there is more to publication than its attendant systems.
The basic units of scientific communication are also beginning to change.

**Null results and lab notebooks**  
Currently scientists communicate results in the form of discrete units called "papers" or "posters" or "conference talks". 
Such research artifacts are post-hoc methods of distilling a series of questions and analyses into a cohesive story.
This distillation is helpful; a paper is easier to read and understand than someone's lab notebook.
However, if the table stakes for official scientific communication are weeks or months of work on the submission, then smaller findings will fall through the cracks.
Few scientists are willing to spend that kind of time to compile and submit a null result, or a result that is low impact but raises new questions.
The effort required to do so would be disproportionate to the benefits.

Fortunately, we live in the 21st century.
The internet gives us the ability to make notes available to to the entire world seamlessly.
Now that distribution costs are negligible, it is no longer infeasible to maintain a repository of null results.
Simple signposts of the form "I tried X, it didn't work, here's my code/experimental description" could save huge amounts of scientist labor by cautioning people away from known pitfalls.

We could even go further and normalize keeping public electronic lab notebooks.
Having a record of the original work would help people trying to extend a paper's results, as they would be able to see the logic behind what is included or left out.
It would also be a useful educational tool. 
Right now students are told to read papers and guess at why authors did things.
In a world of public lab notebooks a class could teach the entire process that led to a finding instead of solely the final product.

**Bug bounties and code review**  
Right now the quality control process for scientific publications is some combination of "three scientists peer reviewed this so it's probably good", "no one I've seen roasted this paper on Twitter yet", and "getting a paper retracted is embarrasing, so there probably aren't any retraction-worthy errors in this paper".
How much do you trust the average paper in your field?
How much more would you trust it if the authors said they will [give $200 to anyone who can find an error](http://www.the100.ci/2020/06/29/red-team-part-1/)?

If you don't like the idea of getting emails from random people on the internet about how you're wrong and should give them money, maybe you'd feel more comfortable with someone you know doing it.
[Code review](https://google.github.io/eng-practices/review/) is a software engineering practice common in industry, but infrequent in academia.
The idea behind code review is that the author of an analysis might miss problems due to their familiarity with the code, while a second person might not.
Code review should also be a part of the peer review process, but given the current state of peer review that's probably a discussion for the future.

On a similar tack, in my [recent paper](https://www.nature.com/articles/s41592-021-01256-7) we[^we] proposed having a collaborator or coauthor whose entire role is to make sure the work is reproducible.
We describe the role more eloquently there: "We envision a reproducibility collaborator as someone outside the primary authors’ research groups who certifies that they were able to reproduce the results of the paper from only the data, models, code and accompanying documentation." 

**Interactive papers**  
Papers now live primarily on websites, which in turn live on computers.
Things that once made sense, like limiting the amount of color in figures or having a strict length limit, make less sense today.
However, we can do much better than "wow, this paper would have cost a lot to print on paper, that figure has five colors!"

We can [make papers interactive](https://distill.pub/).
Why explain an acronym once when you could have optional expansion though the entire paper?
Why not have the corresponding regions in a genome browser light up whenever you hover your cursor over a [Hi-C](https://en.wikipedia.org/wiki/Chromosome_conformation_capture#Hi-C_(all-vs-all)) heatmap?
Why use only 2D plots when you could make a 3D figure that your can rotate in your browser?

For computational analyses, we can go a step further and give readers runnable code within the paper [or attached as a notebook](https://twitter.com/lpachter/status/1441257875225989125).
Reading about an analysis is great, but being able to ask questions and see what changes when you tweak the parameters is even better.

## Conclusion
Where does that leave us?
The probability that the world will decide one day to reject the status quo and adopt some of the systems above is vanishingly small.
Institutions like academia and governments large enough to contend with publishers are too large to move that quickly.

Instead I think the situation is more "the future is already here but not evenly distributed".
In biology, preprints have gone from a novelty to [an appreciable fraction of the total literature](https://asapbio.org/preprint-info/biology-preprints-over-time).
Likewise, having a Twitter account dedicated to tweeting about papers from a field is uncommon, but some individuals already have tens of thousands of followers from doing exactly that.
In fact, many of the ideas listed in this post are already being implemented on some level, but aren't the predominant mode of operation.
All of these ideas have the potential to grow, it's just a matter of seeing what people will decide to use.

Personally the future that I hope for is a preprint -> pseudonymous paid peer review -> overlay journal system.
I think it's close enough to our current institutions to avoid breaking anything while alleviating a lot of the pain points.
But the beauty of living on the edge of the future is that it doesn't matter what I think, you can choose your own favorite scientific system.
So [start your own overlay journal](https://www.episciences.org/), or submit your paper to [Review Commons](https://www.reviewcommons.org/), or come up with something better than anything I've written about.
In a world that is changing every day, go make your own choice for the future happen.

### Further Reading
- ["This has ... been a time of growing divergence between the different roles of academic publishing: as a means of disseminating validated knowledge, as a form of symbolic capital for academic career progression, and as a profitable business enterprise."](https://zenodo.org/record/546100)
- ["Why is it so hard to access science? Because the modern academic publishing model is set up to be difficult and turn a (massive) profit for publishers. The goal is to make money, not disseminate knowledge."](https://nicolebarbaro.substack.com/p/failedjournals)
- ["[The current publication system] has the farcical consequence of universities collectively paying private companies to legally access the collective output of their own work."](https://tribunemag.co.uk/2021/08/we-cant-trust-the-market-with-scientific-knowledge)
- ["Whatever happens to Sci-Hub or Elbakyan, the fact that such a site exists is something of a tragedy."](https://palladiummag.com/2021/09/24/a-world-without-sci-hub/)
- ["...if peer review is so central to the process by which scientific knowledge becomes canonised, it is ironic that science has little to say about whether it works."](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1114539/)
- [More visions of a potential future (though not strictly publication related)](https://atoms.org/scientiae#future-applications)

### Footnotes
[^fyfe]: Read [Fyfe et al.](http://garfield.library.upenn.edu/essays/v4p394y1979-80.pdf) for a more in-depth though fairly UK-centric description).
[^owns]: Technically the journal only receives the copyright for the paper, but given that you can't even [include the published version in your dissertation](https://authorservices.taylorandfrancis.com/publishing-your-research/moving-through-production/copyright-for-journal-authors/), you effectively don't own it.
[^budget]: Publishing companies' revenues don't come solely from the US, so it's not necessarily the case that 20% of NIH funding goes to publisher fees. However, it is the case that you could fund two more NSFs with the amount of revenue journals make.
[^preprints]: Sure, some people publish preprints, but in a preprint-only world there would be no reason to delay an extra X months/years.
[^peerreview]: The way peer review is often conducted also [has issues](https://theconversation.com/the-peer-review-system-for-academic-papers-is-badly-in-need-of-repair-72669), but even if they were fixed the system would still be flawed.
[^good]: For some definition of "good" that combines expected citations, significance of findings, interest to the reviewers, luck, etc.
[^disclaimer]: I've been reading deep learning research for ~6 years now, but I haven't attended any DL conferences myself. This section is my understanding of the system from my spot in computational biology.
[^embargo]: In truth it's a little more complicated than that; some conferences prohibit posting/promotion of non-anonymized papers to maintain double-blindness in the review process.
[^reading]: At least that's what I do, your mileage may vary.
[^AK]: Yes their initials are both AK. No, as far as I know they are not the same person.
[^yelp]: In fact, companies like Yelp have built entire businesses around the thesis "peer review is a useful signal" for a different type of peer review.
[^we]: We in the royal sense, if I remember correctly it's Florian's idea.
[^calibration]: That is to say if a review is very negative it's difficult to tell whether that's a function of the reviewer or the paper unless you have many reviews to compare against.
