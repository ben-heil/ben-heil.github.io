---
layout: post
title: The Past, Present, and Future of Academic Publishing
description: ""
cover-img: "/assets/img/traintracks.jpg"
tags: ['grad school', 'academia', 'journals']
---

## Intro
The paper publication system is simple.
Once scientists have written about their science, they send their work to a company called a publisher.
The publisher then pays other scientists in [internet points](https://publons.com/about/home/) to determine whether it is worth their time to publish.
If the reviewers decide the paper is good enough, the publisher will generously allow the authors to pay the journal to put it on their website.
There is, of course, another fee if the authors want people to be able to read their paper without paying.

Historically, scientists communicated [to the public through books and to each other with letters](http://garfield.library.upenn.edu/essays/v4p394y1979-80.pdf)
Around the 17th century, scientific societies were formed to "[share their results in person, and presumably consume alcohol](https://blogs.scientificamerican.com/information-culture/the-mostly-true-origins-of-the-scientific-journal/)"
Over time, these societies began to publish periodicals, which were the precursors to modern academic journals.
However, the journals we have today are largely a result of the expansion and codification of academia in the decades after WWII[^fyfe].

Present-day academic journals are inefficient due to a combination of historical tradition and profit considerations.
The [costs](https://twitter.com/DGlaucomflecken/status/1484679759829209090) of this inefficiency is largely borne by scientists, and better alternatives exist.
This post discusses the [issues](https://twitter.com/behrenstimb/status/1492269544513150977) in the current system of publication in academia, and how they can be fixed.

## Paper Ownership
When a paper is published in closed-access journals, the journal owns the paper.
In the past, this meant that only people who paid the journal's access fees were able to read papers.
After pushback, and an [NIH mandate](https://en.wikipedia.org/wiki/NIH_Public_Access_Policy), it is now possible to read some publicly funded research for free (after an embargo) through PubMed Central.
The journal still keeps the license, of course, so [only a subset](https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/) of published articles are available for future research.

Even paying an institutional licensing fee doesn't give you free reign.
[Chris Albon once tried](http://partiallyderivative.com/podcast/2017/02/09/mini-episode-data-scraping) to scrape JSTOR articles to a tablet so he could spend a summer backpacking and preparing for his comprehensive exams.
As those familiar with the publishing system would predict, JSTOR threatened sanctions against his university and made him delete all the articles.

Posession of large numbers of academic articles will cause people to show up at your office. 
Possession with the intent to distribute invariably incurs [legal](https://en.wikipedia.org/wiki/Aaron_Swartz) [action](https://www.theverge.com/2018/2/8/16985666/alexandra-elbakyan-sci-hub-open-access-science-papers-lawsuit).

## Time Costs
If all goes well scientists write a paper, submit it, wait, get reviews, address them, wait, then publish.
The median time from having a paper completed enough to submit to submission is about [five months](https://www.biorxiv.org/content/10.1101/2021.03.04.433874v2.full.pdf).
During this time the research is done and the paper is written, but no one other than the authors, reviewers, and the publisher can read it[^preprints].
As a result, the rate of scientific progress is slowed down by months.

This delay falls disproportionately on unestablished scientists.
People with a large network and frequent conference attendance might hear about unpublished results and adjust their research agendas accordingly.
Early career scientists are unlikely to have the same network or funding for conference attendance, so they have to live with an information disadvantage compared to senior scientists.

## Peer Review 
A common argument to justify academic journals is the importance of peer review as a quality control mechanism.
While the covid pandemic has [made that claim questionable](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7358687/), there are more systemic issues that outweigh it even if it were true[^peerreview].

At its core, the current framing of peer review is flawed.
Journals aren't trying to determine how good a paper is, they're trying to determine whether they should publish it.
In an ideal world, these would be the same thing, but in practice they diverge due to perverse incentives.

Authors want your paper to be published after one round of review so that they can spend their time doing science instead of corresponding with journals.
Accordingly, the acceptable responses to review take the form "We enthusiastically agree with reviwer 3's suggestion that we [cite their marginally related articles](https://academic.oup.com/bioinformatics/article/35/18/3217/5304360), and have done so in the revised manuscript."
Disagreements are heavily penalized as they increase the probability that the editor will send the paper out for another round of review.

Reviwers, knowing this, can get away with recommending that the authors cite their paper.
Alternatively, they can emphasize all the flaws in their competitors' paper in an attempt to keep it from getting published.

Because reviewers have no skin in the game, it's challenging to put checks on this kind of malfeasance.
The worst case scenario is that they get reported to the journal and fired from their unpaid job as a reviewer.

Peer review isn't a bad idea, just not well-implemented in the current system.
In a world where peer review was performed [independent to journal submission](#TODO) (or after), these perverse incentives would be greatly reduced.

## Externalized Costs
One reason that the inefficiencies in the modern publishing system aren't more regularly talked about in academia is that many of the costs are [externalized](https://en.wikipedia.org/wiki/Cost_externalizing).
For example the cost of paper access is paid for by taxpayers/nonprofits by way of universities' [indirect costs](https://oamp.od.nih.gov/division-of-financial-advisory-services/indirect-cost-branch/indirect-cost-submission/indirect-cost-definition-and-example) on grants.

Academics largely don't know or care what the cost to access a journal is, unless their [university system](https://www.insidehighered.com/news/2019/03/01/university-california-cancels-deal-elsevier-after-months-negotiations) or [country](https://www.editage.com/insights/norway-joins-the-ranks-of-germany-and-sweden-cancels-subscription-with-elsevier) decides to fight a publisher.
They simply log in from a university IP address or use a VPN and get automatic access.
Taxpayers who are the ones funding research, on the other hand, have to hope the article they want is open access, wait for the article to show up on PubMed Central, or pirate the paper from site like SciHub.

Likewise, the costs of peer review are borne by academics.
As things stand now, peer review is a system where world-class experts provide [billions of dollars](https://researchintegrityjournal.biomedcentral.com/articles/10.1186/s41073-021-00118-2) of consulting to for-profit publishing companies for free.

Another externalized cost of the modern publication system is the increased risk to the scientific record.
Academic publishers own the articles they publish, and it is illegal to create an archive of scientific literature like SciHub.
As a result, there is a single point of failure for the preservation of scientific knowledge.

Publishers surely have backup systems, but how robust are they?
If a smaller journal got [ransomwared](https://www.pbs.org/newshour/nation/why-ransomware-attacks-are-on-the-rise-and-what-can-be-done-to-stop-them), would they have the funds to pay the ransom?
Or would that science be lost forever?

## What do journals provide?
Despite their inefficiency, modern academic publications do play several important roles in furthering science.

On the author side journals act as platforms, allowing scientists a place to disseminate their ideas.
Even if the authors of a paper aren't widely known, if they can write a paper good[^good] enough to get into e.g. Nature or Science, it will get readership.
While increasing readership of certain papers isn't the goal of science per se, citations are a unit of academic currency and journals help mitigate the extent to which they are driven solely by networking ability.

On the reader side, journals act as filters, separating "can't miss" papers from the pack.
[Thousands of biological papers](https://www.biorxiv.org/search/jcode%3Abiorxiv%20limit_from%3A2020-01-01%20limit_to%3A2020-12-31%20numresults%3A10%20sort%3Arelevance-rank%20format_result%3Astandard) are posted each month, making it challenging for scientists to keep track of which papers are relevant, much find the best ones.
Journals coordinate peer review as an additional filtering mechanism that they can get for free.
Even though I don't believe that "is this paper good enough for X journal" is a good framing for peer review, I do think that peer review can be a useful filtering method.

## Alternatives
I believe that journals provide value to the scientific community.
They provide a place to access research, they coordinate peer review, and they publicize the research of scientists who don't have a network to self-promote.
Further, I don't believe in providing criticism without alternatives unless you genuinely believe "nothing" is a better option than the thing you're arguing against.
So how do we achieve the roles played by academic journals while reducing the costs?

### Platforms
While in the past the only ways for scientists to disseminate work was through journals, letters, or conferences, the internet has made distribution much easier.
The number of papers being submitted to preprint servers is [growing rapidly](https://asapbio.org/preprint-info/biology-preprints-over-time), allowing scientists to share their work with the world for free (!).
While preprint servers don't have the same readership base as major journals, removing the barrier to actually reading the work in question greatly expands what is possible.

One example of a preprint+ hybrid model is the conference system in deep learning research[^disclaimer].
DL papers tend to be published as preprints when they are completed,[^embargo] then submitted to conferences instead of journals ([if they are submitted at all](https://twitter.com/fchollet/status/1467023443010928640)).
Conferences like NeurIPS and ICML are the equivalents of Cell or Nature in biology, and are paid close attention to by the field as a whole.
While the conference system [has its](https://www.reddit.com/r/compsci/comments/by4w3/the_cs_conference_reviewing_process_is_broken/) [own problems](https://www.researchgate.net/post/What_is_your_opinion_on_the_current_conference-driven_publication_model_in_CS), it's at least an order of magnitude cheaper for scientists.

Another platform of note is science Twitter.
Though I don't think the format is actually healthy for communicating science (or anything for that matter), it is effective at transmitting ideas to a large number of people.
Some day I want to run an experiment by purchasing a twitter ad for the same price as an article publication fee and see whether the journal or the ad gets more people to read the paper.
I suspect it would be the latter.

Twitter isn't designed to be a platform for discussing and circulating scientific research, but some websites tackle at least parts of that goal.
[Publons](https://publons.com/about/home/), for example, aims to give more credit to peer reviewers.
Due to anonymity, peer reviews historically haven't been rewarded.
Publons gives scientists the ability to point to a trusted source and say "I reviewed X articles in Y journals over the past five years."

### Filters
Every scientific paper is different, and each paper is of more or less interest depending on the person.
Given infinite time, scientists would try to read all papers in existence, but we don't live in that world.
Instead, we try to read as many papers directly pertaining to our research topic as we can while reading only the more important papers for less related topics [^reading].
As things stand, journals are organized to fit this system well.
There are journals dedicated solely to (their opinion of) papers of high general importance, and other journals that focus on the minutiae of a specific topic.

Requiring scientists to skim all the preprinted papers to stay up to date in their field is likely an impractical amount of work.
To avoid this fate, we need to replicate two key functions: aggregation ("what is relevant?"), and peer review ("what is good?").

#### Alternate aggregation
Speaking of impractical amounts of work, let's talk about deep learning research as a field.
Because of its conference-based system and the fast pace of progress, scientists in deep learning actually are reliant on preprints in a field that posts hundreds per day.
To survive such a harsh environment, deep learning communities have responded by building tools to make the firehose of information more manageable.
As a result, several of the best methods I've seen so far come from the deep learning comunity.

Ironically, given the field's drive to automate everything, the current solution is to throw scientist-hours at the problem.
Freelance paper aggregators such as [AK](https://twitter.com/ak92501) and [Aran Komatsuzaki](https://twitter.com/arankomatsuzaki) select the preprints that they think are the most important each day and post about them on Twitter.

A more formal implementation of this same idea is something called an '[overlay journal](https://peeriodicals.com/)'.
Overlay journals are called such because they build on top of the existing publication infrastructure.
Instead of handling peer review, hosting, and so on, overlay journals simply aggregate papers on a certain topic as they are preprinted or published elsewhere.
In the overlay paradigm, individuals are able to create a collection of papers relevant to their field without the costs of starting their own publication system.

Off the beaten path, you can see more creative views of science publication.
Creators like [3Blue1Brown](https://www.youtube.com/watch?v=3s7h2MHQtxc) have shown that you can learn science by seeing it instead of by reading it.
Embracing that thesis, channels like [OMGenomics](https://www.youtube.com/watch?v=-PCKK_nwFdA) and [Yannic Kilcher](https://www.youtube.com/watch?v=iDulhoQ2pro) have sprung up to do walkthroughs of scientific projects and papers.
Instead of just selecting papers that are interesting, such channels add their own interpretation and understanding to the science.

While the methods I've presented so far have all involved manual curation, there are certainly other ways of approaching the aggregation problem.
[Several](https://arxiv-sanity-lite.com/about) [natural language processing](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0158423) [methods](https://elicit.org) exist for finding and aggregating papers.
Such methods are useful in that they're able to take in a larger quantity of papers than a human could, and may be less swayed by a publication's venue or the status the authors writing the paper.
The hard part, as always, lies in getting the algorithm's results to align with intent of the human driving it.

Since paper aggregation is largely a matter of taste, I don't think there is a single answer for what the best alternative aggregation system is.
My guess is that the future will be made up of several different manual and automatic systems, driven by popularity and sutability for particular uses.

#### Peer review
Although its [current form](https://www.youtube.com/watch?v=lt_b4VKBDhI) is inefficient and misaligned with the goals of science, I do believe that peer review has value.

Peer review is a little strange, in that it involves for-profit companies getting consulting hours from scientists with advanced degrees for free.
I'm of the opinion that this is bad, and not just because I'm a grad student who would like more money.
When making a resource free (or very cheap) any system attempting to maximize profits will use it for [anything they can draw value from](https://twitter.com/SwiftOnSecurity/status/1074856487514726400?s=20&t=BV4kfDhsriljMwkR-F9T9w).
While peer review itself isn't a resource that broadly useful, academic labor is.
As a result, pushing back against unpaid peer review via something like [The 450 Movement](https://jamesheathers.medium.com/the-450-movement-1f86132a29bd) is important to avoid normalizing unpaid academic labor.
Otherwise we'll end up with people posting unpaid professor positions or something.
[Oh wait](https://twitter.com/kchoudhu/status/1505040603851239424).

One way to improve the review process is to separate it from the journals making decisions about acceptance.
The peer review process [takes months](https://link.springer.com/article/10.1007/s11192-017-2310-5#Sec7), so having to rereview a paper due to journal fit or expected impact is a huge waste of time.
[Review Commons](https://www.reviewcommons.org/) is developing a way to preventing rereviews (at least for the life sciences).
They coordinate peer review, then send the resulting reviews to their affiliate journals.
While they still reject papers based on [topic scope and perceived importance](https://www.reviewcommons.org/faq/), Review Commons is still moving the field towards journal-agnostic peer review, which is a win in my book.

While Review Commons makes the peer review process more efficient by coordinating journals, [OpenReview](https://openreview.net/) has gone a step further by building an entire platform.
At the moment, this platform is still very tied to individual conferences; works are submitted to a conference and reviewed/accepted/rejected by people affiliated with the conference.
The idea of a peer review platform promises more, though.

With very similar technology, it would be possible to coordinate freelance paid peer review (thankfully, OpenReview is open source).
The majority of peer review today is anonymous and fragmented.
Each journal knows the tone and quality of individuals' reviews, but there are so many journals it is difficult to get an idea of reviewers' calibration[^calibration].
Meanwhile, [Publons](https://publons.com/about/home/) knows who has reviewed for which papers, but not the scores they gave or the content of the reviews.
By centralizing reviews and reviewers a platform could simultaneously shine light on bad behavior, disentangle the peer review and journal acceptance process, and coordinate and enforce reviewer payment.
As an added bonus, by making the system pseudonymous the platform could achieve all these goals while maintaining single- or double-blind peer review.

### What lies beyond?
**Null results and lab notebooks**  
Journals are a remnant of a time when scientific communication was done through words on paper.
The ideas above list how this system could be adapted to be more efficient, but the academic journal isn't the platonic ideal of academic communication.
Right now it can cost [more than US$10,000](https://www.nature.com/articles/d41586-020-03324-y) to publish a paper. 
Given the choice between trying to publish a negative result or paying an appreciable fraction of a grad student's yearly salary, most people would pick the latter.
The internet gives the power of free distribution though.
With that new calculus, what becomes possible?

Current science communication takes the form of discrete chunks called "papers" or "posters" or "conference talks". 
Such artifacts are post-hoc methods of distilling a series of questions and analyses into a cohesive story.
This distillation isn't bad in and of itself; a paper is easier to read and understand than someone's lab notebook.
The drawback comes from the friction of of publishing itself.
Few scientists are willing to spend the weeks or months necessary to compile and submit a null result, or a result that is low impact but raises new questions.
The effort required to do so would be disproportionate to any benefit to their career.

Fortunately, we don't live in the 1800s.
The internet gives us the ability to make notes available to to the entire world seamlessly.
As a result we can publish null results more easily.
If you can ignore distribution costs, it is no longer infeasible to maintain a repository of that sort.
Simple signposts of the form "I tried X, it didn't work, here's my code/experimental description" could save huge amounts of scientist labor by cautioning people away from known pitfalls.

We could go even further and normalize keeping public electronic lab notebooks.
Doing so would both be useful for people trying to extend a paper, as they would be able to see the logic behind what is included or left out.
It would also be a useful educational tool. 
Right now students are told to read papers and guess at why authors did things.
In a world of public lab notebooks a class could teach the entire process that led to the paper instead of solely the final product.

**Bug bounties**  
Right now the quality control process for scientific publications is some combination of "three scientists peer reviewed this so it's probably good", "no one I've seen roasted this paper on Twitter yet", and "getting a paper retracted is embarrasing, so there probably aren't any retraction-worthy errors in this paper".
How much do you trust the average paper in your field?
How much more would you trust it if the authors said they will [give $200 to anyone who can find an error](http://www.the100.ci/2020/06/29/red-team-part-1/)?

**Code Review**  
If you don't like the idea of getting emails from random people on the internet about how you're wrong and should give them money, maybe you'd feel more comfortable with someone you know doing it.
[Code review](https://google.github.io/eng-practices/review/) is a software engineering practice common in industry, but infrequent in academia.
The idea behind code review is that the author of an analysis might miss problems due to their familiarity with the code, while a second person might not.
Code review should also be a part of the peer review process, but given the current state of peer review that's probably a discussion for the future.

On a similar tack, in my [recent paper](https://www.nature.com/articles/s41592-021-01256-7) we[^we] proposed having a collaborator or coauthor whose entire role is to make sure the work is reproducible.
We describe the role more eloquently there: "We envision a reproducibility collaborator as someone outside the primary authorsâ€™ research groups who certifies that they were able to reproduce the results of the paper from only the data, models, code and accompanying documentation." 

**Interactive papers**  
Papers now live primarily on websites, which in turn live on computers.
Things that once made sense, like limiting the amount of color in figures or having a strict length limit, make less sense today.
However, we can do much better than "wow, this paper would have cost a lot to print on paper, that figure has five colors!" by [making papers interactive](https://distill.pub/).
Why explain an acronym once when you could have optional expansion though the entire paper?
Why not have the corresponding regions in a genome browser light up whenever you hover your cursor over a [Hi-C](https://en.wikipedia.org/wiki/Chromosome_conformation_capture#Hi-C_(all-vs-all)) heatmap?
Why use only 2D plots when you could make a 3D figure that your can rotate in your browser?

For computational analyses, we can go a step further and give readers runnable code within the paper [or attached as a notebook](https://twitter.com/lpachter/status/1441257875225989125).
Reading about an analysis is great, but being able to ask questions about what changes if you tweak the parameters is even better.

## My ideal science stack/Conclusion/Synthesis:

- Paper published on a preprint server (at this point it would just be called a paper server)
- Authors Pseudonymous paid peer review 
- Submissions to overlay journals

TODO journals shape fields, what happens when individuals have more say in what journals look like? https://twitter.com/zacharylipton/status/1493818507246678017

### Further Reading
- ["Why is it so hard to access science? Because the modern academic publishing model is set up to be difficult and turn a (massive) profit for publishers. The goal is to make money, not disseminate knowledge."](https://nicolebarbaro.substack.com/p/failedjournals)
- ["This has ... been a time of growing divergence between the different roles of academic publishing: as a means of disseminating validated knowledge, as a form of symbolic capital for academic career progression, and as a profitable business enterprise."](https://zenodo.org/record/546100)
- ["[The current publication system] has the farcical consequence of universities collectively paying private companies to legally access the collective output of their own work."](https://tribunemag.co.uk/2021/08/we-cant-trust-the-market-with-scientific-knowledge)
- ["Whatever happens to Sci-Hub or Elbakyan, the fact that such a site exists is something of a tragedy."](https://palladiummag.com/2021/09/24/a-world-without-sci-hub/)
- [More visions of a potential future (though not strictly publication related)](https://atoms.org/scientiae#future-applications)

### Footnotes
[^fyfe]: Read [Fyfe et al.](http://garfield.library.upenn.edu/essays/v4p394y1979-80.pdf) for a more in-depth though fairly UK-centric description).
[^budget]: Publishing companies' revenues don't come solely from the US, so it's not necessarily the case that 20% of NIH funding goes to publisher fees. However, it is the case that you could fund two more NSFs with the amount of revenue journals make.
[^preprints]: Sure, some people publish preprints, but in a preprint-only world there would be no reason to delay an extra X months/years.
[^peerreview]: The way peer review is often conducted also [has issues](https://theconversation.com/the-peer-review-system-for-academic-papers-is-badly-in-need-of-repair-72669), but even if they were fixed the system would still be flawed.
[^good]: For some definition of "good" that combines expected citations, significance of findings, interest to the reviewers, etc.
[^disclaimer]: I've been reading DL research for ~6 years now, but I haven't attended any DL conferences myself. This section is my read on the situation from an adjacent space.
[^embargo]: It's a little more complicated than that; some conferences prohibit posting/promotion of non-anonymized papers to maintain double-blindness in the review process.
[^reading]: At least that's what I do, your mileage may vary.
[^we]: We in the royal sense, if I remember correctly it's Florian's idea.
[^calibration]: That is to say if a review is very negative it's difficult to tell whether that's a function of the reviewer or the paper unless you have many reviews to compare against.
