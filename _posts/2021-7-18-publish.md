---
layout: post
title: The Past, Present, and Future of Academic Publishing
description: ""
cover-img: "/assets/img/traintracks.jpg"
tags: ['grad school', 'academia', 'publication']
---

## The Past

The scientific publication system is simple.
Once scientists have written a paper about their science, they send their work to a company called a publisher.
The publisher then gives other scientists (called reviewers) [internet points](https://publons.com/about/home/) to determine whether it is worth their time to publish.
If the reviewers decide the paper is good enough, the publisher will generously allow the authors to pay the journal to put the paper on their website.
There is, of course, another fee if the authors want people to be able to read their paper without paying a further toll.

It wasn't always this way, though.

Historically, scientists communicated [to the public through books and communicated to each other with letters](http://garfield.library.upenn.edu/essays/v4p394y1979-80.pdf).
Then, around the 17th century, scientific societies were formed to "[share their results in person, and presumably consume alcohol](https://blogs.scientificamerican.com/information-culture/the-mostly-true-origins-of-the-scientific-journal/)."
Over time, these societies began to publish periodicals, which were the precursors to modern academic journals.
However, the publication system we have today is largely a result of the expansion and codification of academia in the decades after WWII[^fyfe].

The present-day academic publication system is inefficient, and the [costs](https://twitter.com/DGlaucomflecken/status/1484679759829209090) of this inefficiency borne both by scientists and society as a whole.
Fortunately, with the advent of the internet, several alternatives have arisen.
This post discusses [issues](https://twitter.com/behrenstimb/status/1492269544513150977) in the current publication system and what the future of academic publishing might look like.

## The Present

**Paper Ownership**  
When a paper is published in closed-access journals, the publisher owns[^owns] it.
In the past, this meant that only people who paid the journal's access fees were able to read papers.
After pushback (and an [NIH mandate](https://en.wikipedia.org/wiki/NIH_Public_Access_Policy)), it is now possible to read some publicly funded research for free (after an embargo) through PubMed Central.
Of course, the publisher still keeps the copyright, so [only a subset](https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/) of published articles are available for reserach via text mining.

Even paying an institutional licensing fee doesn't give you free rein.
[Chris Albon once tried](http://partiallyderivative.com/podcast/2017/02/09/mini-episode-data-scraping) to scrape JSTOR articles to a tablet so he could spend a summer backpacking and preparing for his [comprehensive exams](https://en.wikipedia.org/wiki/Comprehensive_examination).
As those familiar with the publishing system would predict, JSTOR threatened sanctions against his university and made him delete all the articles.
Possession of large numbers of academic articles will cause people to show up at your office. 
Possession with the intent to distribute invariably incurs [legal](https://en.wikipedia.org/wiki/Aaron_Swartz) [action](https://www.theverge.com/2018/2/8/16985666/alexandra-elbakyan-sci-hub-open-access-science-papers-lawsuit).

**Publication Delays**  
If all goes well, scientists write a paper, submit it, wait, get reviews, address them, wait, then publish.
In biology, the median time between having a paper complete enough to preprint and having it published is about [150 days](https://www.biorxiv.org/content/10.1101/2021.03.04.433874v2.full.pdf).
During this time, the research is done, and the paper is written. However, unless the paper is preprinted, no one other than the authors, reviewers, and the publisher can read it[^preprints].
As a result, the rate of scientific progress is slowed down by months.

**Peer Review**  
Modern peer review is a little strange. 
It involves for-profit companies getting consulting hours from scientists with advanced degrees for free.
I'm of the opinion that this is bad, and not just because I'm a grad student who likes having money.
When making a resource free (or very cheap), any system attempting to maximize profits will use it for [anything they can draw value from](https://twitter.com/SwiftOnSecurity/status/1074856487514726400?s=20&t=BV4kfDhsriljMwkR-F9T9w).
While peer review itself isn't a broadly useful resource, academic labor is.
If we're not careful about giving away scientist-hours for free, we might end up with universities posting unpaid professor positions or something.
[Oh, wait](https://twitter.com/kchoudhu/status/1505040603851239424).

In addition to flaws in pricing, there are also issues with the current framing of peer review.
Publishers aren't trying to determine how good a paper is, they're trying to determine [whether they should publish it](https://authorservices.wiley.com/Reviewers/journal-reviewers/what-is-peer-review/index.html).
In an ideal world, these would be the same thing, but in practice they diverge due to perverse incentives.

Authors want their paper to be published after one round of review so that they can spend their time doing science instead of corresponding with reviewers.
Accordingly, the acceptable response to reviews takes the form "[We thank the reviewer](https://openreview.net/search?term=%22we+thank+the+reviewer%22&group=all&content=all&source=all) for their suggestion that we [cite their marginally related articles](https://academic.oup.com/bioinformatics/article/35/18/3217/5304360), and have done so in the revised manuscript."
Disagreements are heavily penalized as they increase the probability of a negative assessment from the reviewer and a second round of "suggestions."

Reviewers, knowing this, can get away with misbehavior, like increasing their citation count by recommending that the authors cite their papers.
Alternatively, if the author is a competitor, a reviewer can emphasize the flaws in the paper in an attempt to keep it from getting published.

Because reviewers have no skin in the game, it's challenging to put checks on this kind of malfeasance.
The worst-case scenario is that they get reported and removed from their unpaid job as a reviewer for that journal.

Ultimately, the current system of peer review [infrequently](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3001285) changes the results of a paper but frequently [causes delays](https://www.nature.com/articles/s41599-021-00920-9/figures/1).

**Externalized Costs**  
One reason that the inefficiencies in the modern publishing system aren't more visible is that many of the costs are [externalized](https://en.wikipedia.org/wiki/Cost_externalizing).
For example, the cost of paper access is paid for by taxpayers/nonprofits through universities' [indirect costs](https://oamp.od.nih.gov/division-of-financial-advisory-services/indirect-cost-branch/indirect-cost-submission/indirect-cost-definition-and-example) on grants.

Academics generally aren't presented with the cost to access a journal unless their [university system](https://www.insidehighered.com/news/2019/03/01/university-california-cancels-deal-elsevier-after-months-negotiations) or [country](https://www.editage.com/insights/norway-joins-the-ranks-of-germany-and-sweden-cancels-subscription-with-elsevier) decides to fight a publisher.
They simply log in from a university IP address or use a VPN and get automatic access.
Meanwhile, taxpayers funding the research have to hope the article they want is open access, wait for the article to appear on PubMed Central, pay hundreds of dollars for a subscription to the journal, or pirate the paper from a site like SciHub.

Likewise, the costs of peer review are borne by academics.
As things stand now, peer review is a system where world-class experts provide [billions of dollars](https://researchintegrityjournal.biomedcentral.com/articles/10.1186/s41073-021-00118-2) of consulting to for-profit publishing companies for free.

A less-discussed cost of the modern publication system is the increased risk to the scientific record.
Academic publishers own the copyright for the articles they publish, so it is illegal to create an archive of scientific literature.
As a result, there is a single point of failure for the preservation of scientific knowledge.
Publishers surely have backup systems, but how robust are they?
If a smaller journal got [ransomwared](https://www.pbs.org/newshour/nation/why-ransomware-attacks-are-on-the-rise-and-what-can-be-done-to-stop-them), would they have the funds to pay the ransom?
Or would that science be lost forever?

## The Future
Flaws aside, modern academic journals play several vital roles.

For authors, journals act as platforms, allowing scientists a place to disseminate their ideas.
Even if the authors of a paper aren't widely known, if they can write a paper good[^good] enough to get into, e.g., Nature or Science, it will get readership.
While increasing the readership of specific papers isn't the goal of science per se, citations are a unit of academic currency, and journals help mitigate the extent to which academic success is driven solely by networking ability.

For readers, journals act as filters, separating "can't miss" papers from the pack.
[Thousands of biological papers](https://www.biorxiv.org/search/jcode%3Abiorxiv%20limit_from%3A2020-01-01%20limit_to%3A2020-12-31%20numresults%3A10%20sort%3Arelevance-rank%20format_result%3Astandard) are posted each month, making it challenging for scientists to keep track of which papers are relevant.
To alleviate that issue, journals coordinate peer review as an additional filtering mechanism to augment the editor's discernment with that of experts in the field.

In general, I don't believe in providing criticism without alternatives, so how do we fill the roles played by academic journals while reducing the drawbacks?

### Platforms  
While in the past the only way for scientists to publicize their work was through journals, letters, or conferences, the internet has made distribution much more accessible.
The number of papers being submitted to preprint servers is [growing rapidly](https://asapbio.org/preprint-info/biology-preprints-over-time), allowing scientists to share their work with the world for free (!).
While preprint servers don't necessarily have the same prestige as major journals, removing the barrier to actually reading the work in question opens the door for alternative platforms.

For example, deep learning research uses a hybrid model of preprints and conferences[^disclaimer].
Deep learning papers tend to be published as preprints when they are completed,[^embargo] then submitted to conferences instead of journals ([if they are submitted at all](https://twitter.com/fchollet/status/1467023443010928640)).
Conferences like NeurIPS and ICML are the equivalents of Cell or Nature in biology and are paid close attention to by the field as a whole.
While the conference system [has its](https://www.reddit.com/r/compsci/comments/by4w3/the_cs_conference_reviewing_process_is_broken/) [own problems](https://www.researchgate.net/post/What_is_your_opinion_on_the_current_conference-driven_publication_model_in_CS), it's at least an order of magnitude cheaper for scientists.

Another platform of note is Science Twitter.
Though I don't think the format is healthy for communicating science (or anything for that matter), it is effective at transmitting ideas to a large number of people[^twitter].

Twitter isn't designed to be a platform for discussing and circulating scientific research, but some websites take on parts of that goal.
[Publons](https://publons.com/about/home/), for example, aims to give more credit to peer reviewers.
Due to anonymity, peer reviews historically haven't been rewarded.
Publons gives scientists the ability to point to a trusted source and say, "I reviewed X articles in Y journals."

### Filters
Given infinite time, scientists would try to read all papers in existence, but we don't live in that world.
Instead, we try to read as many papers on our research topic as we can while reading only the essential articles for less related topics[^reading].
As things stand, journals are organized to fit this system well.
There are journals dedicated solely to (in their opinion) papers of high general importance and other journals that focus on the minutiae of a specific topic.

Requiring scientists to skim all the preprinted papers to stay up to date in their field is likely an impractical amount of work.
To avoid this fate, we need to replicate two key functions: aggregation ("what is relevant?"), and peer review ("what is good?").

**Alternate aggregation**  
Speaking of impractical amounts of work, let's talk about deep learning research as a field.
Because of its conference-based system and the fast pace of progress, deep learning scientists rely on preprints in a field that posts hundreds per day.
To survive such a dynamic environment, deep learning communities have responded by building tools to make the firehose of information more manageable.
As a result, several of the best methods for aggregation I've seen so far come from the deep learning community.

Ironically, given the field's drive to automate tasks done by humans, one of the current solutions is to throw scientist-hours at the problem.
Freelance paper aggregators such as [AK](https://twitter.com/ak92501) and [Aran Komatsuzaki](https://twitter.com/arankomatsuzaki)[^AK] select the preprints that they think are the most important each day and post about them on Twitter.

A more formal implementation of this same idea is something called an "[overlay journal](https://peeriodicals.com/)."
Overlay journals are called such because they build on top of the existing publication infrastructure.
Instead of handling peer review, hosting, and so on, overlay journals simply aggregate papers on a particular topic as they are preprinted or published elsewhere.
In the overlay paradigm, individuals can create a collection of papers relevant to their field without the costs of starting their own publication system.

Further afield, creators like [3Blue1Brown](https://www.youtube.com/watch?v=3s7h2MHQtxc) show that you can learn science by seeing it instead of by reading it.
Embracing that thesis, channels like [OMGenomics](https://www.youtube.com/watch?v=-PCKK_nwFdA) and [Yannic Kilcher](https://www.youtube.com/watch?v=iDulhoQ2pro) have sprung up to do walkthroughs of scientific projects and papers.
Instead of just selecting interesting papers, such channels add their own interpretation and understanding to the science.

While the methods I've presented so far have all involved manual curation, there are other ways of approaching the aggregation problem.
[Several](https://elicit.org) [natural language processing](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0158423) [methods](https://arxiv-sanity-lite.com/about) exist for finding and aggregating papers.
Such methods can take in a larger quantity of papers than a human and may be less swayed by a publication's venue or the status of the authors writing the paper.
The hard part, as always, lies in getting the algorithm's results to align with the intent of the human driving it.

Since paper aggregation is largely a matter of taste, I don't think there is a single answer for the best alternative aggregation system.
I believe future scientists will employ several different manual and automatic systems, driven by popularity and suitability for particular uses.

**Peer review**  
Although peer review is a helpful signal about a paper's quality[^yelp], in its [current form](https://www.youtube.com/watch?v=lt_b4VKBDhI), it is inefficient and misaligned with the goals of science.

One potential method for aligning the incentives of peer reviewers, authors, and publishers would be to [pay peer reviewers](https://jamesheathers.medium.com/the-450-movement-1f86132a29bd).
Paying reviewers encourages publishers to value reviewers' time by avoiding sending papers for peer review that are unlikely to be accepted.
It also encourages reviewers to behave well to avoid risking their payment.
You could even imagine a system where grad students looking to gain domain knowledge and augment their stipends could review papers rather than teaching classes.

Another promising way to improve the review process would be to separate it from publishers' decisions to accept or reject the paper.
The peer review process [takes months](https://link.springer.com/article/10.1007/s11192-017-2310-5#Sec7), and having to rereview a paper due to journal fit or expected impact exacerbates the issue.
[Review Commons](https://www.reviewcommons.org/) is developing a way to prevent rereviews (at least for the life sciences).
They coordinate peer review, then send the resulting reviews to their affiliate journals.
While they still reject papers based on [topic scope and perceived importance](https://www.reviewcommons.org/faq/), Review Commons is still moving the field towards journal-agnostic peer review, which is a win in my book.

While Review Commons makes the peer review process more efficient by coordinating journals, [OpenReview](https://openreview.net/) has gone a step further by building an entire platform.
Currently, this platform is tied to individual conferences; works are submitted to a conference and reviewed by people affiliated with the conference.
The idea of a peer review platform promises more, though.

With very similar technology, it would be possible to coordinate freelance paid peer review (thankfully, OpenReview is open source).
The majority of peer review today is anonymous and fragmented.
Each journal knows the quality of individuals' reviews, but there are so many journals that it is challenging to get an idea of reviewers' calibration[^calibration].
Meanwhile, [Publons](https://publons.com/about/home/) knows who has reviewed for which journals but not their scores.
By centralizing reviews and reviewers, a platform could simultaneously shine light on bad behavior, disentangle the peer review and journal acceptance process, and coordinate reviewer payment.
As a bonus, by making the system pseudonymous, such a platform could achieve all these goals while maintaining single- or double-blind peer review.


### The article of the future
As the world continues to grow and change, the systems around academic publication will change with it.
But there is more to publication than its attendant systems.
The basic units of scientific communication are also beginning to change.

**Null results and lab notebooks**  
Currently, scientists communicate results in the form of discrete units like papers, posters, and conference talks. 
Such research artifacts are post hoc methods of distilling a series of questions and analyses into a cohesive story.
This distillation is helpful; a paper is easier to read and understand than someone's lab notebook.
However, if the table stakes for official scientific communication are weeks or months of work on the submission, then more minor findings will fall through the cracks.
Few scientists are willing to spend that kind of time to compile and submit a null result or a result that is low impact but raises new questions.
The effort required to do so would be disproportionate to the benefits.

Fortunately, we live in the 21st century.
The internet gives us the ability to make notes available to the entire world with little friction.
Now that distribution costs are negligible, it is no longer infeasible to maintain a repository of null results.
Simple signposts of the form "I tried X, but it didn't work. Here's my code and experimental description" could save vast amounts of scientist labor by steering people away from known pitfalls.

We could even go further and normalize keeping public electronic lab notebooks.
Having a record of the original work would help people trying to extend a paper's results, as they would be able to see the logic behind what is included or left out.
It would also be a valuable educational tool. 
Students are currently told to read papers and speculate about why authors did what they did.
In a world of public lab notebooks, a class could teach the entire process that led to a finding instead of solely the final product.

**Bug bounties and code review**  
Right now, the quality control process for scientific publications is some combination of "three scientists peer reviewed this, so it's probably good," "no one I've seen roasted this paper on Twitter yet," and "getting a paper retracted is embarrassing, so there probably aren't any retraction-worthy errors in this paper."
How much do you trust the average paper in your field?
How much more would you trust it if the authors said they would [give $200 to anyone who can find an error](http://www.the100.ci/2020/06/29/red-team-part-1/)?

If you don't like getting emails from random people on the internet about how you're wrong, maybe you'd feel more comfortable with someone you know doing it.
[Code review](https://google.github.io/eng-practices/review/) is a software engineering practice common in industry but infrequent in academia.
The idea behind code review is that the author of an analysis might miss problems due to their familiarity with the code, while a second person might not[^codereview].

On a similar tack, in my [recent paper](https://www.nature.com/articles/s41592-021-01256-7) we[^we] proposed having a collaborator or coauthor whose role is to make sure the work is reproducible.
We describe the role more eloquently there: "We envision a reproducibility collaborator as someone outside the primary authors' research groups who certifies that they were able to reproduce the results of the paper from only the data, models, code and accompanying documentation." 

**Interactive papers**  
Papers now live primarily on websites, which in turn live on computers.
Things that once made sense, like limiting the amount of color in figures or having a strict length limit, make less sense today.
However, we can do much better than "Wow, this paper would have cost a lot to print on paper. That figure has five colors!"

We can [make papers interactive](https://distill.pub/).
Why explain an acronym once when you could have optional expansion though the entire paper?
Why not have the corresponding regions in a genome browser light up whenever you hover your cursor over a [Hi-C](https://en.wikipedia.org/wiki/Chromosome_conformation_capture#Hi-C_(all-vs-all)) heatmap?
Why use only 2D plots when you could make a 3D figure that you can rotate in your browser?

For computational analyses, we can go a step further and give readers runnable code within the paper [or attached as a notebook](https://twitter.com/lpachter/status/1441257875225989125).
Reading about an analysis is great, but being able to ask questions and see what changes when you tweak the parameters is even better.

## Conclusion
Where does that leave us?
The probability that the world will decide one day to reject the status quo and adopt all of the systems above is vanishingly small.
Institutions like academia and governments large enough to contend with publishers are too large to move that quickly.

Instead, I think the situation is more "the future is already here but not evenly distributed."
In biology, preprints have gone from a novelty to [an appreciable fraction of the total literature](https://asapbio.org/preprint-info/biology-preprints-over-time).
Likewise, having a Twitter account dedicated to tweeting about papers from a field is uncommon, but some individuals already have tens of thousands of followers from doing precisely that.
In fact, many of the ideas listed in this post are already being implemented on some level but aren't the predominant mode of operation.
All of these ideas have the potential to grow; it's just a matter of seeing what people will decide to use.

Personally, the future that I hope for is a preprint → pseudonymous paid peer review → overlay journal system.
I think it's close enough to our current institutions to avoid breaking anything while alleviating many pain points.
But the beauty of living on the edge of the future is that regardless of what I think, you can choose your own ideal scientific system.
So [start an overlay journal](https://www.episciences.org/), or submit your paper to [Review Commons](https://www.reviewcommons.org/), or come up with something better than anything I've written about.
In a world that is changing every day, go make your own choice for the future happen.

### Further Reading
- [Arcadia, a recently founded research organization, implements several of the ideas above in their publishing platform.](https://research.arcadiascience.com/reimagining-scientific-publishing)
- [More visions of a potential future (though not strictly publication related)](https://atoms.org/scientiae#future-applications)
- ["This has ... been a time of growing divergence between the different roles of academic publishing: as a means of disseminating validated knowledge, as a form of symbolic capital for academic career progression, and as a profitable business enterprise."](https://zenodo.org/record/546100)
- ["Whatever happens to Sci-Hub or Elbakyan, the fact that such a site exists is something of a tragedy."](https://palladiummag.com/2021/09/24/a-world-without-sci-hub/)
- ["...if peer review is so central to the process by which scientific knowledge becomes canonised, it is ironic that science has little to say about whether it works."](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1114539/)

### Footnotes
[^fyfe]: Read [Fyfe et al.](http://garfield.library.upenn.edu/essays/v4p394y1979-80.pdf) for a more in-depth though fairly UK-centric description).
[^owns]: Technically, the journal only receives the copyright for the paper, but given that you can't even [include the published version in your dissertation](https://authorservices.taylorandfrancis.com/publishing-your-research/moving-through-production/copyright-for-journal-authors/), you effectively don't own it.
[^preprints]: Sure, some people publish preprints, but in a preprint-only world there would be no reason to delay an extra X months/years.
[^peerreview]: The way peer review is often conducted also [has issues](https://theconversation.com/the-peer-review-system-for-academic-papers-is-badly-in-need-of-repair-72669), but even if they were fixed, the system would still be flawed.
[^good]: For some definition of "good" that combines expected citations, the significance of findings, interest to the reviewers, luck, etc.
[^disclaimer]: I've been reading deep learning research for ~6 years now, but I haven't attended any DL conferences myself. This section is my understanding of the system from my spot in computational biology.
[^embargo]: In truth it's a little more complicated than that; some conferences prohibit posting/promotion of non-anonymized papers to maintain double-blindness in the review process.
[^twitter]: Someday I want to run an experiment by purchasing a Twitter ad for the same price as an article publication fee and see whether the journal or the ad gets more people to read the paper. I suspect it would be the latter.
[^reading]: At least that's what I do. Your mileage may vary.
[^AK]: Yes, their initials are both AK. No, they are not the same person as far as I know.
[^yelp]: In fact, companies like Yelp have built entire businesses around the thesis "peer review is a useful signal" for a different type of peer review.
[^codereview]: Code review should also be a part of the peer review process, but given the current state of peer review that's probably a discussion for the future.
[^we]: We in the royal sense, if I remember correctly, it's Florian's idea.
[^calibration]: That is to say, if a review is very negative, it's difficult to tell whether that's a function of the reviewer or the paper unless you have many reviews to compare against.
